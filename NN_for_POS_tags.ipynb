{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UdrGvvcQbMZn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.util import ngrams\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#initialize start and end tokens, and ''unknown'' tag\n",
    "START = '<s>'\n",
    "END = '</s>'\n",
    "UNK = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a feed-forward neural network classifier to predict the POS tag of a word in its context. The input\n",
    "should be the word embedding for the center word concatenated with the word embeddings for words\n",
    "in a context window. We’ll define a context window as the sequence of words containing w words\n",
    "to either side of the center word and including the center word itself, so the context window contains\n",
    "1 + 2w words in total. For example, if w = 1 and the word embedding dimensionality is d, the total\n",
    "dimensionality of the input will be 3d. For words near the sentence boundaries, pad the sentence with\n",
    "beginning-of-sentence and end-of-sentence characters (< s> and < /s>). The word embeddings should be randomly initialized and learned along with all other parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is the concatenation of word embeddings in the context window, with the word to be tagged in the center. We use a single hidden layer of width 128 with a tanh nonlinearity. The hidden layer is then fed to an affine transformation which will produce scores for all possible POS tags. Finally, we use a softmax transformation on the scores to produce a probability distribution over tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_table('data/twpos-data/twpos-train.tsv', sep=r'\\t', header=None, skip_blank_lines=False)\n",
    "dev = pd.read_table('data/twpos-data/twpos-dev.tsv', sep=r'\\t', header=None, skip_blank_lines=False)\n",
    "devtest = pd.read_table('data/twpos-data/twpos-devtest.tsv', sep=r'\\t', header=None, skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the POS Tagger Neural Network\n",
    "class POS_Tagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, window, embedding_dim, hidden_dim):\n",
    "        super(POS_Tagger, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.fc1 = nn.Linear(embedding_dim * (1 + 2 * window), hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_dim, tag_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = embeds.view(embeds.size(0), -1)\n",
    "        out = self.tanh(self.fc1(embeds))\n",
    "        tag_space = self.fc2(out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "VOCAB_SIZE = len(VOCAB_TO_IDX)\n",
    "TAG_SIZE = len(TAG_TO_IDX)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "WINDOW = 1\n",
    "LEARING_RATE = 0.1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = POS_Tagger(VOCAB_SIZE, TAG_SIZE, WINDOW, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARING_RATE)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = create_dataloader(train, WINDOW, BATCH_SIZE)\n",
    "dev_dl = create_dataloader(dev, WINDOW, BATCH_SIZE)\n",
    "devtest_dl = create_dataloader(devtest, WINDOW, BATCH_SIZE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(EPOCHS):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for context_tensor, target in train_dl:\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(context_tensor)\n",
    "        loss = loss_function(tag_scores, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Eval phase\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for context_tensor, target in dev_dl:\n",
    "            tag_scores = model(context_tensor)\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    dev_acc = accuracy_score(all_targets, all_preds)\n",
    "    print(f'Epoch {epoch}: Dev Accuracy: {round(dev_acc, 3)}, Total Loss: {round(total_loss, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We obtain that 82.14% is the best accuracy achieved when testing finally on DEVTEST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a feature function we concatenate to each vector the number of characters in its corresponding word\n",
    "emb = {}\n",
    "for word in train_dataset[0].values:\n",
    "    emb[word] = [random.uniform(-0.1, 0.1) for _ in range(50)] + [len(word)]\n",
    "    \n",
    "emb['</s>'] = [random.uniform(-0.1, 0.1) for _ in range(51)]\n",
    "emb['UUUNKKK'] = [random.uniform(-0.1, 0.1) for _ in range(51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "Na0pz_HVwQmB",
    "outputId": "66f73937-0b01-4d78-d263-9374b26574f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10274/10274 [00:00<00:00, 50642.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_feat = RandomEmbeddingsDataSet(1)\n",
    "train_loader_feat = DataLoader(train_data_feat, batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector embeddings for test set using the embeddings for training\n",
    "test_emb = {}\n",
    "for word in test_dataset[0].values:\n",
    "    if word not in train_dataset[0].values:\n",
    "        test_emb[word] = [0]*51\n",
    "    else:\n",
    "        test_emb[word] = emb[word]\n",
    "test_emb['</s>'] = emb['</s>']\n",
    "test_emb['UUUNKKK'] = emb['UUUNKKK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4650/4650 [00:00<00:00, 40323.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_feat = TestEmbeddingsDataSet(1)\n",
    "test_loader_feat = DataLoader(test_data_feat, batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetFeat(nn.Module):\n",
    "    def __init__(self, w):\n",
    "        super(NeuralNetFeat, self).__init__()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(((2*w)+1)*51 ,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 25),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.05)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        u = self.stack(input)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "model = NeuralNetFeat(1)\n",
    "early_stopping = EarlyStopping(patience=2, path='my_model_checkpoint.pt')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = 0.9)\n",
    "train_errs, test_errs = train(10, model, criterion, optimizer, train_loader_feat, test_loader_feat)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
